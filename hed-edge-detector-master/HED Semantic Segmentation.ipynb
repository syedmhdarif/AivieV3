{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e21a5383",
   "metadata": {},
   "source": [
    "# boundary semantic detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb0cc05",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf0155d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "# import Augmentor\n",
    "from dataloader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.data import AUTOTUNE\n",
    "\n",
    "from torchvision import transforms\n",
    "caffe_root = r'C:\\Users\\alext\\Jupyter\\caffe' # this file is expected to be in {caffe_root}/examples/hed/\n",
    "\n",
    "from modules import caffe\n",
    "import sys\n",
    "sys.path.insert(0, caffe_root + 'python')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d12048f",
   "metadata": {},
   "source": [
    "### Data Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e3adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\alext\\Desktop\\dataset\\AHP'\n",
    "\n",
    "# original =sorted(glob(os.path.join(path, 'AHP3k1/*')))\n",
    "# mask = sorted(glob(os.path.join(path, 'AHP3kMasking1/*')))\n",
    "original_path = r'C:\\Users\\alext\\Desktop\\dataset\\AHP\\AHP3k1'\n",
    "mask_path = r'C:\\Users\\alext\\Desktop\\dataset\\AHP\\AHP3kMasking1'\n",
    "edge_path = r'C:\\Users\\alext\\Desktop\\dataset\\AHP\\AHP3kHED1'\n",
    "\n",
    "ori = [os.path.join(original_path,x) for x in os.listdir(original_path)]\n",
    "mask = [os.path.join(mask_path,x) for x in os.listdir(mask_path)]\n",
    "edge = [os.path.join(edge_path,x) for x in os.listdir(edge_path)]\n",
    "\n",
    "len(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b8e013",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5b87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NUM = 1\n",
    "SMOOTH = 1e-15\n",
    "INIT_LR = 1e-2\n",
    "#TEST_SPLIT = 0.05\n",
    "VAL_SPLIT = 0.2\n",
    "IMAGE_SIZE = 224\n",
    "EPOCHS = 2\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186c2ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the images and masks, and return the data in batches, augmented optionally.\n",
    "\n",
    "dataset = DataLoader(image_paths=ori,\n",
    "                     mask_paths=mask,\n",
    "                     image_size=(255, 255),\n",
    "                     crop_percent=0.8,\n",
    "                     channels=(3, 1),\n",
    "                     augment=True,\n",
    "                     compose=False,\n",
    "                     seed=47)\n",
    "\n",
    "dataset = dataset.data_batch(batch_size=32,\n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ac34a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_SIZE = len(ori)\n",
    "VALID_SIZE = int(VAL_SPLIT * TOTAL_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42099447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split IMAGES data into train and validation set\n",
    "train_x, valid_x = train_test_split(ori, test_size=VALID_SIZE, random_state=42)\n",
    "\n",
    "#split MASKS data into train and validation set\n",
    "train_y, valid_y = train_test_split(mask, test_size=VALID_SIZE, random_state=42)\n",
    "\n",
    "train_z, valid_z = train_test_split(edge, test_size=VALID_SIZE, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff6ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebfcfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = ImageDataGenerator(rescale = 1/255)\n",
    "# valid = ImageDataGenerator(rescale = 1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750fdd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = train.flow_from_directory(r'C:\\Users\\alext\\Desktop\\dataset\\AHP\\AHP3k1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f747a9",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087aa783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv3D\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import pyimage\n",
    "from modules.keras_models import MobileNetV3SmallSegmentation\n",
    "from tensorflow.image import ResizeMethod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708e0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_images(imagePath, labelPath):\n",
    "    # read the image from disk, decode it, convert the data type to\n",
    "    # floating point, and resize it\n",
    "    image = tf.io.read_file(imagePath)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "\n",
    "    #image = tf.image.per_image_standardization(image)\n",
    "    \n",
    "    label = tf.io.read_file(labelPath)\n",
    "    label = tf.image.decode_png(label, channels=3)\n",
    "    label = tf.image.rgb_to_grayscale(label)\n",
    "    label = tf.image.convert_image_dtype(label, dtype=tf.float32)\n",
    "    #label = tf.image.per_image_standardization(label)\n",
    "    label = tf.image.resize(label, (224, 224))\n",
    "\n",
    "    # return the image and the label\n",
    "    return (image, label)\n",
    "\n",
    "def augment_using_ops(images, labels):\n",
    "    images = tf.image.random_brightness(images, 0.2)\n",
    "    images = tf.image.random_contrast(images, 0.5, 2.0)\n",
    "    images = tf.image.random_saturation(images, 0.75, 1.25)\n",
    "    images = tf.image.random_hue(images, 0.1)\n",
    "    return (images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d34b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94d5470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true = tf.keras.layers.Flatten()(y_true)\n",
    "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    return (2. * intersection + SMOOTH) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + SMOOTH)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1.0 - dice_coef(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad75b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU_loss(y_true, y_pred):\n",
    "\n",
    "    numerator = tf.reduce_sum(y_true * y_pred)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred) - numerator \n",
    "\n",
    "    jac =  numerator / (denominator + tf.keras.backend.epsilon())\n",
    "\n",
    "    return 1 - jac\n",
    "\n",
    "def IoU(y_true, y_pred, t=0.5):\n",
    "\n",
    "\n",
    "    y_pred_ = tf.cast(y_pred > t, dtype=tf.int32)\n",
    "    y_true = tf.cast(y_true, dtype=tf.int32)\n",
    "\n",
    "    TP = tf.math.count_nonzero(y_pred_ * y_true)\n",
    "    FP = tf.math.count_nonzero(y_pred_ * (y_true - 1))\n",
    "    FN = tf.math.count_nonzero((y_pred_ - 1) * y_true)\n",
    "\n",
    "    jac = tf.cond(tf.greater((TP + FP + FN), 0), lambda: TP / (TP + FP + FN),\n",
    "                  lambda: tf.cast(0.000, dtype='float64'))\n",
    "\n",
    "    return jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 224\n",
    "img_cols = 224\n",
    "\n",
    "input_shape = (img_rows, img_cols, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90872ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c47e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# # model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "# model.add(tf.keras.layers.Reshape((224, 224, 3), input_shape=(224, 224, 3)))\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', padding='same',  input_shape=(batch_size,224, 224, 3),data_format='channels_first'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# # model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "# # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# # model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "# # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# # model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# # model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# # model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(1024, input_shape=(3072,), activation=\"sigmoid\"))\n",
    "# model.add(tf.keras.layers.Dense(512, activation=\"sigmoid\"))\n",
    "# # model.add(Dense(128,  input_shape=(255),activation='softmax'))\n",
    "\n",
    "# # print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89ef958",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MobileNetV3SmallSegmentation(alpha=1.0, shape=(224, 224), n_class=CLASS_NUM,\n",
    "                 avg_pool_kernel=(11, 11), avg_pool_strides=(4, 4),\n",
    "                 resize_method=ResizeMethod.BILINEAR, backbone='small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e948da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt = Adam(lr=INIT_LR, clipvalue = 5.0)\n",
    "#opt = Adam(lr=INIT_LR, clip_norm = 1.0)\n",
    "\n",
    "#m = tf.keras.metrics.MeanIoU(CLASS_NUM)\n",
    "metrics = [dice_coef,\"accuracy\", IoU]\n",
    "#metrics = [dice_coef, \"accuracy\"]\n",
    "\n",
    "model.compile(loss=dice_loss, optimizer=opt, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17371db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = len(train_x)//batch_size\n",
    "valid_steps = len(valid_x)//batch_size\n",
    "\n",
    "if len(train_x) % batch_size != 0:\n",
    "    train_steps += 1\n",
    "    \n",
    "if len(valid_x) % batch_size != 0:\n",
    "    valid_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa40ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "\n",
    "# build the training dataset and data input pipeline\n",
    "trainDS = tf.data.Dataset.from_tensor_slices((ori,mask))\n",
    "\n",
    "#trainDS = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "trainDS = (trainDS\n",
    "    .map(load_images, num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .batch(batch_size)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "#trainDS = trainDS.map(lambda x, y: (normalization_layer(x), y))\n",
    "#image_batch, labels_batch = next(iter(normalized_ds))\n",
    "\n",
    "# build the validation dataset and data input pipeline\n",
    "valDS = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
    "valDS = (valDS\n",
    "    .map(load_images, num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .batch(batch_size)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "#valDS = valDS.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "# build the testing dataset and data input pipeline\n",
    "#testDS = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
    "#testDS = (testDS\n",
    "    #.map(load_images, num_parallel_calls=AUTOTUNE)\n",
    "    #.cache()\n",
    "    #.batch(BATCH)\n",
    "    #.prefetch(AUTOTUNE)\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aeddf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efd71b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging = BaggingClassifier(KNeighborsClassifier(),\n",
    "                             max_samples=0.5, max_features=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6459ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = model.fit(\n",
    "    trainDS,\n",
    "    validation_data=valDS.repeat(),\n",
    "    epochs=100,\n",
    "    steps_per_epoch=len(train_x)//batch_size,\n",
    "    validation_steps=len(valid_x)//batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349f3942",
   "metadata": {},
   "source": [
    "### HED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b053d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pyimage\n",
    "import cv2\n",
    "from os.path import dirname, join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a87810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropLayer(object):\n",
    "    def __init__(self, params, blobs):\n",
    "        # initialize our starting and ending (x, y)-coordinates of\n",
    "        # the crop\n",
    "        self.startX = 0\n",
    "        self.startY = 0\n",
    "        self.endX = 0\n",
    "        self.endY = 0\n",
    "        \n",
    "    def getMemoryShapes(self, inputs):\n",
    "        # the crop layer will receive two inputs -- we need to crop\n",
    "        # the first input blob to match the shape of the second one,\n",
    "        # keeping the batch size and number of channels\n",
    "        (inputShape, targetShape) = (inputs[0], inputs[1])\n",
    "        (batchSize, numChannels) = (inputShape[0], inputShape[1])\n",
    "        (H, W) = (targetShape[2], targetShape[3])\n",
    "        # compute the starting and ending crop coordinates\n",
    "        self.startX = int((inputShape[3] - targetShape[3]) / 2)\n",
    "        self.startY = int((inputShape[2] - targetShape[2]) / 2)\n",
    "        self.endX = self.startX + W\n",
    "        self.endY = self.startY + H\n",
    "        # return the shape of the volume (we'll perform the actual\n",
    "        # crop during the forward pass\n",
    "        return [[batchSize, numChannels, H, W]]\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "    # use the derived (x, y)-coordinates to perform the crop\n",
    "        return [inputs[0][:, :, self.startY:self.endY, self.startX:self.endX]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b6aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our serialized edge detector from disk\n",
    "print(\"[INFO] loading edge detector...\")\n",
    "\n",
    "protoPath = join(dirname(r\" C:\\Users\\alext\\Jupyter\\modules\\hed-edge-detector-master\\edge_detector\"),\n",
    "                 r\"C:\\Users\\alext\\Jupyter\\modules\\hed-edge-detector-master\\deploy.prototxt\")\n",
    "\n",
    "modelPath = r\"modules\\hed-edge-detector-master\\hed_pretrained_bsds.caffemodel\"\n",
    "\n",
    "net = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "# register our new layer with the model\n",
    "cv2.dnn_registerLayer(\"Crop\", CropLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8024b25e",
   "metadata": {},
   "source": [
    "### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d74cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the input image and grab its dimensions\n",
    "for items in \n",
    "image = cv2.imread(r'C:\\Users\\alext\\Desktop\\1kAHP\\Background\\image1.PNG')\n",
    "(H, W) = image.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e22e950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f47c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a blob out of the input image for the Holistically-Nested\n",
    "# Edge Detector\n",
    "blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size=(W, H),\n",
    "mean=(104.00698793, 116.66876762, 122.67891434),\n",
    "swapRB=False, crop=False)\n",
    "\n",
    "# set the blob as the input to the network and perform a forward pass\n",
    "# to compute the edges\n",
    "print(\"[INFO] performing holistically-nested edge detection...\")\n",
    "net.setInput(blob)\n",
    "hed = net.forward()\n",
    "hed = cv2.resize(hed[0, 0], (W, H))\n",
    "hed = (255 * hed).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61070fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "canny = cv2.Canny(image,100,200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303bc349",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"image\", image)\n",
    "cv2.imshow(\"HED\", hed)\n",
    "cv2.imshow(\"canny\", canny)\n",
    "\n",
    "# cv2.imwrite(r\"C:\\Users\\alext\\Desktop\\1kAHP\\hed2.jpg\",hed)\n",
    "# cv2.imwrite(r\"C:\\Users\\alext\\Desktop\\1kAHP\\canny.jpg\",canny)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa534ef5",
   "metadata": {},
   "source": [
    "### Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af307cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "\n",
    "# build the training dataset and data input pipeline\n",
    "trainDS = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "#trainDS = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "trainDS = (trainDS\n",
    "    .map(load_images, num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .batch(BATCH)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "#trainDS = trainDS.map(lambda x, y: (normalization_layer(x), y))\n",
    "#image_batch, labels_batch = next(iter(normalized_ds))\n",
    "\n",
    "# build the validation dataset and data input pipeline\n",
    "valDS = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
    "valDS = (valDS\n",
    "    .map(load_images, num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .batch(BATCH)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "#valDS = valDS.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "# build the testing dataset and data input pipeline\n",
    "#testDS = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
    "#testDS = (testDS\n",
    "    #.map(load_images, num_parallel_calls=AUTOTUNE)\n",
    "    #.cache()\n",
    "    #.batch(BATCH)\n",
    "    #.prefetch(AUTOTUNE)\n",
    "#)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27b73ae",
   "metadata": {},
   "source": [
    "### Augmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c10ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Augmentor.Pipeline(r'C:\\Users\\alext\\Desktop\\dataset\\AHP\\AHP3k1')\n",
    "\n",
    "\n",
    "# Add operations to the pipeline as normal:\n",
    "p.rotate(probability=1, max_left_rotation=5, max_right_rotation=5)\n",
    "p.flip_left_right(probability=0.5)\n",
    "p.zoom_random(probability=0.5, percentage_area=0.8)\n",
    "p.flip_top_bottom(probability=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92dab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create two instances with the same arguments\n",
    "data_gen_args = dict(featurewise_center=True,\n",
    "                     featurewise_std_normalization=True,\n",
    "                     rotation_range=90,\n",
    "                     width_shift_range=0.1,\n",
    "                     height_shift_range=0.1,\n",
    "                     zoom_range=0.2)\n",
    "\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c4820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe12566c",
   "metadata": {},
   "source": [
    "## STACKING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c19dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e588ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=1)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a8ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a stacking ensemble of models\n",
    "def get_stacking():\n",
    "    # define the base models\n",
    "    level0 = list()\n",
    "    level0.append(('knn', KNeighborsRegressor()))\n",
    "    level0.append(('cart', DecisionTreeRegressor()))\n",
    "    level0.append(('svm', SVR()))\n",
    "    # define meta learner model\n",
    "    level1 = LinearRegression()\n",
    "    # define the stacking ensemble\n",
    "    model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ed7df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    models['knn'] = KNeighborsRegressor()\n",
    "    models['cart'] = DecisionTreeRegressor()\n",
    "    models['svm'] = SVR()\n",
    "    models['stacking'] = get_stacking()\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b0e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b487f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "train_z, train_z = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447480d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=None, min_samples_split=2,\n",
    "    random_state=0)\n",
    "clf.fit(train_y,train_z)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
