{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## TFLite inference\n",
    "\n",
    "Test TensorFlow Lite inference on an input image."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "from os.path import join\r\n",
    "import tensorflow as tf\r\n",
    "import pickle\r\n",
    "from sklearn import model_selection\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn.ensemble import VotingClassifier\r\n",
    "from sklearn import datasets\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch\r\n",
    "import numpy as np\r\n",
    "import torch.utils.data\r\n",
    "from STEAL.models.casenet import casenet101 as CaseNet101\r\n",
    "from STEAL.coarse_to_fine.input_reader import InputReaderSemMatDemo\r\n",
    "from STEAL.contours import ContourBox\r\n",
    "\r\n",
    "import cv2\r\n",
    "\r\n",
    "import STEAL.utils.vis_utils as vs_utils\r\n",
    "import os\r\n",
    "from STEAL.utils.VisualizerBox import VisualizerBox\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Load TFLite model and allocate tensors.\r\n",
    "\r\n",
    "model1= r\"mobilenetv2_unet_edge_quant.tflite\"\r\n",
    "\r\n",
    "interpreter = tf.lite.Interpreter(model_path=model1)\r\n",
    "interpreter.allocate_tensors()\r\n",
    "\r\n",
    "# Get input and output tensors.\r\n",
    "input_details = interpreter.get_input_details()\r\n",
    "output_details = interpreter.get_output_details()\r\n",
    "\r\n",
    "print(input_details)\r\n",
    "print(output_details)\r\n",
    "\r\n",
    "interpreter.allocate_tensors()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'name': 'input_image', 'index': 0, 'shape': array([  1, 224, 224,   3]), 'shape_signature': array([ -1, 224, 224,   3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "[{'name': 'Identity', 'index': 183, 'shape': array([  1, 224, 224,   1]), 'shape_signature': array([ -1, 224, 224,   1]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test on an image"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#PATH = \"Z://AIVie//Patent//siamese-contrastive-pyimage//experiment//Foreground//dataset//AHP//train//dataset//zoomed//images//COCO_val2014_000000532812_sp_00.png\"\r\n",
    "PATH = r\"C:\\Users\\alext\\Desktop\\dataset\\AHP\\reserved\\JPEGImages\\COCO_train2014_000000015546_sp_00.jpg\"\r\n",
    "#PATH = \"test_images//t7.jpg\"\r\n",
    "INPUT_SIZE = (224, 224)\r\n",
    "\r\n",
    "test_img = cv2.imread(PATH)\r\n",
    "shp = test_img.shape\r\n",
    "test_img = test_img[:,:,::-1]\r\n",
    "test_img = cv2.resize(test_img, INPUT_SIZE)\r\n",
    "\r\n",
    "\r\n",
    "# Test model on the input image.\r\n",
    "input_shape = input_details[0]['shape']\r\n",
    "input_data = np.array((test_img.astype(np.float32) / 255.0)[np.newaxis, :, :, :])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_data.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "interpreter.set_tensor(input_details[0]['index'], input_data)\r\n",
    "\r\n",
    "interpreter.invoke()\r\n",
    "\r\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\r\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\r\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\r\n",
    "print(output_data.shape)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Profile execution"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import time\r\n",
    "\r\n",
    "n = 100\r\n",
    "start_time = time.time()\r\n",
    "for _ in range(n):\r\n",
    "    interpreter.invoke()\r\n",
    "\r\n",
    "print(\"FPS:\", round(n/(time.time() - start_time), 2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize predictions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import PIL\r\n",
    "threshold = 100\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "out_img = np.squeeze(output_data)\r\n",
    "out_img = cv2.resize(out_img, (shp[1],shp[0]))\r\n",
    "\r\n",
    "print(out_img.shape)\r\n",
    "print(cv2.cvtColor(out_img, cv2.COLOR_GRAY2BGR).shape)\r\n",
    "\r\n",
    "test_img = cv2.imread(PATH)\r\n",
    "\r\n",
    "\r\n",
    "plt.figure(figsize=(20,10))\r\n",
    "plt.axis('off')\r\n",
    "plt.imshow(cv2.resize(test_img, (shp[1],shp[0])))\r\n",
    "plt.imshow(out_img*255 > threshold, alpha=0.3)\r\n",
    "plt.savefig(r'C:\\Users\\alext\\Desktop\\dataset\\AHP\\result\\AHPwithZoom_Ori_dicecoef_pic1.jpg')\r\n",
    "\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import cv2\r\n",
    "import random\r\n",
    "\r\n",
    "\r\n",
    "image1 = cv2.imread(r'C:\\Users\\alext\\Desktop\\dataset\\AHP\\result\\AHPwithZoom_dicecoef_pic1.jpg')\r\n",
    "image2 = cv2.imread(r'C:\\Users\\alext\\Desktop\\dataset\\AHP\\result\\AHPwithZoom_16k_dicecoef_pic5.jpg')\r\n",
    "image3 = cv2.imread(r'C:\\Users\\alext\\Desktop\\dataset\\AHP\\result\\AHPwithZoom_Ori_dicecoef_pic1.jpg')\r\n",
    "image4 = cv2.imread(r'C:\\Users\\alext\\Desktop\\dataset\\AHP\\result\\AHPwithZoom_16k_dicecoef_pic7.jpg')\r\n",
    "\r\n",
    "\r\n",
    "fig = plt.figure(figsize=(20, 20))\r\n",
    "\r\n",
    "# f, axs = plt.subplots(1,2,figsize=(20,20))\r\n",
    "\r\n",
    "# ax1 = plt.subplot(1, 2, 1), plt.imshow(image1, 'gray')\r\n",
    "# ax2 = plt.subplot(1, 2, 2), plt.imshow(image2, 'gray')\r\n",
    "\r\n",
    "# fig = plt.figure()\r\n",
    "# ax1 = fig.add_subplot(221)\r\n",
    "# ax2 = fig.add_subplot(222)\r\n",
    "# ax1.title.set_text('First Plot')\r\n",
    "# ax2.title.set_text('Second Plot')\r\n",
    "# plt.imshow(image1, 'gray')\r\n",
    "# plt.imshow(image2, 'gray')\r\n",
    "\r\n",
    "#image 1\r\n",
    "fig.add_subplot(2, 1, 1)\r\n",
    "\r\n",
    "plt.imshow(image3)\r\n",
    "plt.axis('off')\r\n",
    "plt.title(\"Before expand\",fontsize = 40)\r\n",
    "\r\n",
    "#image 2\r\n",
    "fig.add_subplot(2, 1, 2)\r\n",
    "plt.imshow(image4)\r\n",
    "plt.axis('off')\r\n",
    "plt.title(\"After expand\",fontsize = 40)\r\n",
    "\r\n",
    "# #image 3\r\n",
    "# fig.add_subplot(2,2,3)\r\n",
    "# plt.imshow(image3)\r\n",
    "# plt.axis('off')\r\n",
    "# plt.title(\"Before expand\",fontsize = 40)\r\n",
    "\r\n",
    "# #image 4\r\n",
    "# fig.add_subplot(2,2,4)\r\n",
    "# plt.imshow(image4)\r\n",
    "# plt.axis('off')\r\n",
    "# plt.title(\"After expand\",fontsize = 40)\r\n",
    "\r\n",
    "fig.savefig('Expand_diff2.png', dpi=fig.dpi)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib.image as mpimg\r\n",
    "\r\n",
    "def process(filename: str=None) -> None:\r\n",
    "    \"\"\"\r\n",
    "    View multiple images stored in files, stacking vertically\r\n",
    "\r\n",
    "    Arguments:\r\n",
    "        filename: str - path to filename containing image\r\n",
    "    \"\"\"\r\n",
    "    image = mpimg.imread(filename)\r\n",
    "    plt.figure()\r\n",
    "    plt.imshow(image)\r\n",
    "\r\n",
    "images = os.listdir(r'C:\\Users\\alext\\Desktop\\dataset\\AHP\\result')\r\n",
    "\r\n",
    "\r\n",
    "for file in images:\r\n",
    "    file = os.path.join(r'C:\\Users\\alext\\Desktop\\dataset\\AHP\\result',file)\r\n",
    "    process(file)\r\n",
    "\r\n",
    "plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)\r\n",
    "plt.xlabel(\"common X\")\r\n",
    "plt.ylabel(\"common Y\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test on Video"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import time\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "Model_Path = r\"C:\\Users\\alext\\Jupyter\\mobilenetv2_unet_edge_quant.tflite\"\r\n",
    "input_path = (r'C:\\Users\\alext\\Desktop\\dataset\\HED2.mp4')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=Model_Path)\r\n",
    "interpreter.allocate_tensors()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "alpha = 0.5\r\n",
    "beta = (1.0 - alpha)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "cap = cv2.VideoCapture(input_path)\r\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\r\n",
    "\r\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\r\n",
    "writer = None\r\n",
    "\r\n",
    "while (True):\r\n",
    "    \r\n",
    "    ok, frame = cap.read()\r\n",
    "\r\n",
    "    if not ok:\r\n",
    "        break\r\n",
    "\r\n",
    "    ################\r\n",
    "    #model start\r\n",
    "    resize_img = cv2.resize(frame, (224, 224), interpolation=cv2.INTER_CUBIC)\r\n",
    "\r\n",
    "    input_shape = input_details[0]['shape']\r\n",
    "    input_data = np.array((resize_img.astype(np.float32) / 255.0)[np.newaxis, :, :, :])\r\n",
    "\r\n",
    "    #Test model\r\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\r\n",
    "    interpreter.invoke()\r\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\r\n",
    "        \r\n",
    "    #Masking\r\n",
    "    masking = np.squeeze(output_data)\r\n",
    "    masking = cv2.resize(masking, (frame.shape[1],frame.shape[0]))\r\n",
    "    masking= cv2.cvtColor(masking, cv2.COLOR_GRAY2BGR)\r\n",
    "    masking = masking * 255\r\n",
    "            \r\n",
    "    #Add masking to Video\r\n",
    "    dst = cv2.addWeighted(frame,0.8,masking,0.6,0.0,dtype=cv2.CV_8U) \r\n",
    "\r\n",
    "\r\n",
    "    \r\n",
    "    ##################### \r\n",
    "    #writer\r\n",
    "    if writer is None:\r\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\r\n",
    "\r\n",
    "        writer = cv2.VideoWriter(r'C:\\Users\\alext\\Desktop\\output.mp4', fourcc, 30,(frame.shape[1],frame.shape[0]), True)\r\n",
    "\r\n",
    "    if writer is not None:\r\n",
    "        \r\n",
    "        writer.write(dst)\r\n",
    "    ##############################\r\n",
    "    # visualisation\r\n",
    "    cv2.namedWindow(\"cv_image\", cv2.WINDOW_NORMAL)\r\n",
    "    cv2.imshow(\"cv_image\",dst)\r\n",
    "\r\n",
    "\r\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q') :\r\n",
    "        break\r\n",
    "        \r\n",
    "    if cv2.waitKey(1) & 0xFF == ord('p'):\r\n",
    "        cv2.waitKey(-1) #wait until any key is pressed\r\n",
    "\r\n",
    "if writer is not None:\r\n",
    "    writer.release()\r\n",
    "\r\n",
    "    \r\n",
    "# result.release()\r\n",
    "cap.release()\r\n",
    "cv2.destroyAllWindows()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cap = cv2.VideoCapture(input_path)\r\n",
    "ok, frame_image = cap.read()\r\n",
    "original_image_height, original_image_width, _ = frame_image.shape\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "interpreter": {
   "hash": "e94a8b5e93aa01e41e6e7fd5eed958095e4de698ea2869d3915701e87360bf95"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}